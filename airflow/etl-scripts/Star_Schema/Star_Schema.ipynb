{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3daf49c2c9d30c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T09:32:20.818470Z",
     "start_time": "2025-07-04T09:32:19.693526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99,898 rows\n"
     ]
    }
   ],
   "source": [
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "IN_CSV   = \"D:\\Digital_Analytics\\Cleaned data\\Cleaned_Nov.csv\"   # adjust to your file\n",
    "OUT_DIR  = \"D:\\Digital_Analytics\\Star_schema_tables\"                       # where to write the star-CSV files\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "df = pd.read_csv(IN_CSV, parse_dates=[\"event_time\"])\n",
    "print(f\"Loaded {len(df):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179bd165c7a18c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T09:42:42.010826Z",
     "start_time": "2025-07-04T09:42:41.996438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['event_time', 'event_type', 'product_id', 'category_id',\n",
      "       'category_code', 'brand', 'price', 'user_id', 'user_session',\n",
      "       'is_purchase', 'hour_of_day', 'day_of_week', 'is_weekend'],\n",
      "      dtype='object')\n",
      "datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)             # make sure the column is really called 'timestamp'\n",
    "print(df['event_time'].dtype)  # see what pandas thinks it is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147309af4d0bff39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:08:37.434700Z",
     "start_time": "2025-07-04T10:08:37.376743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_date rows: 1\n"
     ]
    }
   ],
   "source": [
    "# ---- robust dim_date build (works with datetime64[ns, UTC]) ------------------\n",
    "\n",
    "# 1️⃣  Make sure event_time is datetime64\n",
    "df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], errors=\"coerce\")\n",
    "\n",
    "# 2️⃣  Floor to midnight so we keep datetime64 dtype\n",
    "dates = df[\"event_time\"].dt.floor(\"D\")          # keeps timezone info if any\n",
    "\n",
    "# 3️⃣  Create dimension\n",
    "dim_date = (\n",
    "    pd.DataFrame({\"date\": dates})\n",
    "      .drop_duplicates(\"date\")\n",
    "      .assign(\n",
    "          date_sk    = lambda x: x[\"date\"].dt.strftime(\"%Y%m%d\").astype(int),\n",
    "          year       = lambda x: x[\"date\"].dt.year,\n",
    "          month      = lambda x: x[\"date\"].dt.month,\n",
    "          day_of_week= lambda x: x[\"date\"].dt.dayofweek,\n",
    "          is_weekend = lambda x: x[\"date\"].dt.dayofweek.isin([5, 6])\n",
    "      )\n",
    "      .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "# 4️⃣  Save to CSV ready for BigQuery LOAD\n",
    "dim_date.to_csv(f\"{OUT_DIR}/dim_date.csv\", index=False)\n",
    "print(\"dim_date rows:\", len(dim_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a2e9077151d7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:16:40.328126Z",
     "start_time": "2025-07-04T10:16:39.582488Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_session = (\n",
    "    df.groupby(\"user_session\")[\"event_time\"]\n",
    "      .agg(session_start = \"min\",\n",
    "           session_end   = \"max\",\n",
    "           n_events      = \"count\")\n",
    "      .assign(duration_s=lambda x: (x[\"session_end\"]-x[\"session_start\"])\n",
    "                                   .dt.total_seconds())\n",
    "      .reset_index()\n",
    "      .rename(columns={\"user_session\": \"session_sk\"})\n",
    ")\n",
    "dim_session.to_csv(f\"{OUT_DIR}/dim_session.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c586f3f5acab5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:16:59.278199Z",
     "start_time": "2025-07-04T10:16:58.987652Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_product = (\n",
    "    df[[\"product_id\", \"category_id\", \"category_code\", \"brand\"]]\n",
    "      .drop_duplicates()\n",
    "      .rename(columns={\"product_id\": \"product_sk\"})\n",
    ")\n",
    "dim_product.to_csv(f\"{OUT_DIR}/dim_product.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbec67bf7b41f523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:43:14.620329Z",
     "start_time": "2025-07-04T11:43:10.128502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 22,443 rows with unique names → D:\\Digital_Analytics\\Star_schema_tables/dim_user.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── Build dim_user with unique synthetic names ──────────────────────────────\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "# 1️⃣  Get distinct user IDs\n",
    "dim_user = (\n",
    "    df[[\"user_id\"]]\n",
    "      .drop_duplicates()\n",
    "      .rename(columns={\"user_id\": \"user_sk\"})\n",
    "      .sort_values(\"user_sk\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 2️⃣  Generate UNIQUE fake names\n",
    "fake.unique.clear()                       # reset uniqueness pool\n",
    "dim_user[\"user_name\"] = [\n",
    "    fake.unique.name() for _ in range(len(dim_user))\n",
    "]\n",
    "\n",
    "# 3️⃣  Save to CSV\n",
    "out_path = f\"{OUT_DIR}/dim_user.csv\"\n",
    "dim_user.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {len(dim_user):,} rows with unique names → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf1fabb341a13ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:35:05.133799Z",
     "start_time": "2025-07-04T10:35:05.098993Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1️⃣  Build dim_event_type WITH a numeric surrogate key\n",
    "codes, uniques = pd.factorize(df[\"event_type\"], sort=True)\n",
    "\n",
    "dim_event_type = (\n",
    "    pd.DataFrame({\"event_type\": uniques})\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\": \"event_type_sk\"})   # 0,1,2… become surrogate key\n",
    ")\n",
    "\n",
    "# 2️⃣  Add that key back onto the main dataframe\n",
    "df[\"event_type_sk\"] = codes     # codes array aligns 1-to-1 with df rows\n",
    "\n",
    "# 3️⃣  Save the dimension\n",
    "dim_event_type.to_csv(f\"{OUT_DIR}/dim_event_type.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da8ef248ba4b351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:35:43.069895Z",
     "start_time": "2025-07-04T10:35:42.075576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_events rows: 99898\n"
     ]
    }
   ],
   "source": [
    "# map date → date_sk\n",
    "date_map = dict(zip(dim_date[\"date\"], dim_date[\"date_sk\"]))\n",
    "df[\"date_sk\"] = df[\"event_time\"].dt.floor(\"D\").map(date_map)\n",
    "\n",
    "fact_events = df[[\n",
    "    \"user_session\",         # session_sk  (FK)\n",
    "    \"user_id\",              # user_sk     (FK)\n",
    "    \"product_id\",           # product_sk  (FK)\n",
    "    \"event_type_sk\",        # FK → dim_event_type\n",
    "    \"date_sk\",              # FK → dim_date\n",
    "    \"price\",\n",
    "    \"is_purchase\"\n",
    "]].rename(columns={\n",
    "    \"user_session\": \"session_sk\",\n",
    "    \"user_id\":      \"user_sk\",\n",
    "    \"product_id\":   \"product_sk\"\n",
    "})\n",
    "\n",
    "\n",
    "fact_events.to_csv(f\"{OUT_DIR}/fact_events.csv\", index=False)\n",
    "print(\"fact_events rows:\", len(fact_events))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
